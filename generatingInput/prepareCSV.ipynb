{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separar RUT de Nombre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been processed successfully.\n"
     ]
    }
   ],
   "source": [
    "def separate_profesor_data():\n",
    "    # Read the CSV file with semicolon delimiter\n",
    "    df = pd.read_csv('Guia_FIA.csv', delimiter=';', encoding='utf-8')\n",
    "    \n",
    "    # Create new columns for RUT and Professor name\n",
    "    df['RUT'] = ''\n",
    "    df['Profesor_Nombre'] = ''\n",
    "    \n",
    "    # Process only rows where 'Profesor' column is not empty and contains a pattern matching a RUT\n",
    "    mask = df['Profesor'].notna() & df['Profesor'].str.contains(r'\\d+-', na=False)\n",
    "    \n",
    "    for idx in df[mask].index:\n",
    "        profesor_str = df.at[idx, 'Profesor']\n",
    "        # Split by first space after the RUT pattern\n",
    "        parts = profesor_str.split(' ', 1)\n",
    "        if len(parts) == 2:\n",
    "            df.at[idx, 'RUT'] = parts[0]\n",
    "            df.at[idx, 'Profesor_Nombre'] = parts[1]\n",
    "    \n",
    "    # Drop the original 'Profesor' column\n",
    "    df = df.drop('Profesor', axis=1)\n",
    "    \n",
    "    # Reorder columns to put RUT and Profesor_Nombre after the fifth column\n",
    "    cols = df.columns.tolist()\n",
    "    # Ensure RUT and Profesor_Nombre are only inserted once\n",
    "    cols = cols[:5] + ['RUT', 'Profesor_Nombre'] + [col for col in cols[5:] if col not in ['RUT', 'Profesor_Nombre']]\n",
    "    df = df[cols]\n",
    "    \n",
    "    # Write the modified DataFrame to a new CSV file\n",
    "    df.to_csv('outputN1_Guia_FIA.csv', sep=';', index=False, encoding='utf-8')\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    result_df = separate_profesor_data()\n",
    "    print(\"CSV file has been processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separar Capacidad de Inscritos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been processed successfully.\n"
     ]
    }
   ],
   "source": [
    "def capacity_and_enrolled():\n",
    "    # Read the output from the previous step\n",
    "    df = pd.read_csv('outputN1_Guia_FIA.csv', delimiter=';', encoding='utf-8')\n",
    "    \n",
    "    # Create new columns\n",
    "    df['Vacantes'] = ''\n",
    "    df['Inscritos'] = ''\n",
    "    \n",
    "    # Process only rows where 'Dic (Cupo/Insc)' column is not empty\n",
    "    mask = df['Dic (Cupo/Insc)'].notna()\n",
    "    \n",
    "    for idx in df[mask].index:\n",
    "        cupo_str = df.at[idx, 'Dic (Cupo/Insc)']\n",
    "        # Extract numbers between parentheses\n",
    "        if 'Nor (' in cupo_str:\n",
    "            numbers = cupo_str.split('(')[1].split(')')[0]\n",
    "            if '/' in numbers:\n",
    "                vacantes, inscritos = numbers.split('/')\n",
    "                df.at[idx, 'Vacantes'] = int(vacantes)\n",
    "                df.at[idx, 'Inscritos'] = int(inscritos)\n",
    "    \n",
    "    # Drop the original column\n",
    "    df = df.drop('Dic (Cupo/Insc)', axis=1)\n",
    "    \n",
    "    # Reorder columns to put Vacantes and Inscritos after the second column\n",
    "    cols = df.columns.tolist()\n",
    "    vacantes_index = cols.index('Vacantes')\n",
    "    inscritos_index = cols.index('Inscritos')\n",
    "    cols = cols[:2] + ['Vacantes', 'Inscritos'] + [col for col in cols[2:] if col not in ['Vacantes', 'Inscritos']]\n",
    "    df = df[cols]\n",
    "    \n",
    "    # Write the modified DataFrame to a new CSV file\n",
    "    df.to_csv('outputN2_Guia_FIA.csv', sep=';', index=False, encoding='utf-8')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Execute the function\n",
    "if __name__ == \"__main__\":\n",
    "    result_df = capacity_and_enrolled()\n",
    "    print(\"CSV file has been processed successfully.\")\n",
    "    # Print first few rows to verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separar NombreSala y Capacidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been processed successfully.\n"
     ]
    }
   ],
   "source": [
    "def separate_classroom_data():\n",
    "    # Read the output from the previous step\n",
    "    df = pd.read_csv('outputN2_Guia_FIA.csv', delimiter=';', encoding='utf-8')\n",
    "    \n",
    "    # Create new columns\n",
    "    df['Sala'] = ''\n",
    "    df['Capacidad'] = ''\n",
    "    \n",
    "    # Process only rows where 'Sala(Cap)' column is not empty\n",
    "    mask = df['Sala(Cap)'].notna()\n",
    "    \n",
    "    for idx in df[mask].index:\n",
    "        sala_str = df.at[idx, 'Sala(Cap)']\n",
    "        if '(' in sala_str and ')' in sala_str:\n",
    "            # Split into sala and capacidad\n",
    "            sala = sala_str.split('(')[0]\n",
    "            capacidad = sala_str.split('(')[1].split(')')[0]\n",
    "            df.at[idx, 'Sala'] = sala\n",
    "            if capacidad.isdigit():  # Only convert to int if it's a number\n",
    "                df.at[idx, 'Capacidad'] = int(capacidad)\n",
    "    \n",
    "    # Drop the original column\n",
    "    df = df.drop('Sala(Cap)', axis=1)\n",
    "    \n",
    "    # Reorder columns to put Sala and Capacidad where Sala(Cap) was\n",
    "    cols = df.columns.tolist()\n",
    "    cols = [col for col in cols if col not in ['Sala', 'Capacidad']]\n",
    "    sala_cap_position = 10  # La posición mencionada (11) menos 1 por el índice base 0\n",
    "    cols = cols[:sala_cap_position] + ['Sala', 'Capacidad'] + cols[sala_cap_position:]\n",
    "    df = df[cols]\n",
    "    \n",
    "    # Write the modified DataFrame to a new CSV file\n",
    "    df.to_csv('outputN3_Guia_FIA.csv', sep=';', index=False, encoding='utf-8')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Execute the function\n",
    "if __name__ == \"__main__\":\n",
    "    result_df = separate_classroom_data()\n",
    "    print(\"CSV file has been processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horas Impartidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been processed successfully.\n"
     ]
    }
   ],
   "source": [
    "def obtain_hours():\n",
    "    # Read the output from the previous step\n",
    "    df = pd.read_csv('outputN3_Guia_FIA.csv', delimiter=';', encoding='utf-8')\n",
    "    \n",
    "    # Create new column for hours\n",
    "    df['Cantidad Horas'] = 0\n",
    "    \n",
    "    # Process only rows where 'Detalle Hora' column is not empty\n",
    "    mask = df['Detalle Hora'].notna()\n",
    "    \n",
    "    def calculate_hours(time_range):\n",
    "        try:\n",
    "            # Split the time range and extract start and end times\n",
    "            start_str, end_str = time_range.split(' - ')\n",
    "            \n",
    "            # Convert times to datetime objects for easier calculation\n",
    "            start_time = datetime.strptime(start_str, '%H:%M')\n",
    "            end_time = datetime.strptime(end_str, '%H:%M')\n",
    "            \n",
    "            # Calculate the difference in hours\n",
    "            diff = end_time - start_time\n",
    "            hours = diff.seconds / 3600\n",
    "            \n",
    "            return int(hours)\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    # Apply the calculation to each row with time data\n",
    "    df.loc[mask, 'Cantidad Horas'] = df.loc[mask, 'Detalle Hora'].apply(calculate_hours)\n",
    "    \n",
    "    # Reorder columns to put Cantidad Horas after Detalle Hora\n",
    "    cols = df.columns.tolist()\n",
    "    detalle_hora_index = cols.index('Detalle Hora')\n",
    "    cols = [col for col in cols if col != 'Cantidad Horas']\n",
    "    cols.insert(detalle_hora_index + 1, 'Cantidad Horas')\n",
    "    df = df[cols]\n",
    "    \n",
    "    # Write the modified DataFrame to a new CSV file\n",
    "    df.to_csv('outputN4_Guia_FIA.csv', sep=';', index=False, encoding='utf-8')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Execute the function\n",
    "if __name__ == \"__main__\":\n",
    "    result_df = obtain_hours()\n",
    "    print(\"CSV file has been processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expandir Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been processed successfully.\n"
     ]
    }
   ],
   "source": [
    "def expand_data():\n",
    "    # Read the output from the previous step\n",
    "    df = pd.read_csv('outputN4_Guia_FIA.csv', delimiter=';', encoding='utf-8')\n",
    "    \n",
    "    # First group of columns to propagate until new Asig. Par. or end marker\n",
    "    course_columns = ['Asig. Par.', 'Asig. Nombre', 'Vacantes', 'Inscritos', 'Carreras']\n",
    "    \n",
    "    # Second group of columns to propagate until new activity/professor\n",
    "    activity_columns = ['Actividad', 'RUT', 'Profesor_Nombre']\n",
    "    \n",
    "    # Initialize variables for current values\n",
    "    current_course = {}\n",
    "    current_activity = {}\n",
    "    \n",
    "    # Process the DataFrame row by row\n",
    "    for i in range(len(df)):\n",
    "        # Check if we've reached the end marker\n",
    "        if df.at[i, 'Asig. Par.'] == 'Fin Guía Académica por Facultades':\n",
    "            break\n",
    "            \n",
    "        # Process course-level information\n",
    "        if pd.notna(df.at[i, 'Asig. Par.']):\n",
    "            # New course found, update current values\n",
    "            for col in course_columns:\n",
    "                current_course[col] = df.at[i, col]\n",
    "        else:\n",
    "            # Propagate course information\n",
    "            for col in course_columns:\n",
    "                df.at[i, col] = current_course.get(col, '')\n",
    "                \n",
    "        # Process activity-level information\n",
    "        has_new_activity = pd.notna(df.at[i, 'Actividad']) and df.at[i, 'Actividad'] != current_activity.get('Actividad', '')\n",
    "        has_new_professor = pd.notna(df.at[i, 'RUT']) and df.at[i, 'RUT'] != current_activity.get('RUT', '')\n",
    "        \n",
    "        if has_new_activity or has_new_professor:\n",
    "            # New activity or professor found, update current values\n",
    "            current_activity = {}\n",
    "            for col in activity_columns:\n",
    "                if pd.notna(df.at[i, col]):\n",
    "                    current_activity[col] = df.at[i, col]\n",
    "        else:\n",
    "            # Propagate activity information if we have stored values\n",
    "            for col in activity_columns:\n",
    "                if pd.isna(df.at[i, col]) and col in current_activity:\n",
    "                    df.at[i, col] = current_activity[col]\n",
    "    \n",
    "    # Write the modified DataFrame to a new CSV file\n",
    "    df.to_csv('outputN5_Guia_FIA.csv', sep=';', index=False, encoding='utf-8')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Execute the function\n",
    "if __name__ == \"__main__\":\n",
    "    result_df = expand_data()\n",
    "    print(\"CSV file has been processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminar Dia/Bloques repetidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_17128\\2063543194.py:30: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  processed_df = pd.concat([processed_df, block_data], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been processed successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_schedule():\n",
    "    # Read the output from the previous step\n",
    "    df = pd.read_csv('outputN5_Guia_FIA.csv', delimiter=';', encoding='utf-8')\n",
    "    \n",
    "    # Create a copy of the DataFrame to store the processed results\n",
    "    processed_df = pd.DataFrame(columns=df.columns)\n",
    "    \n",
    "    # Get unique combinations of Asig. Par. and RUT to identify professor blocks\n",
    "    unique_blocks = df[df['RUT'].notna()][['Asig. Par.', 'Actividad', 'RUT']].drop_duplicates()\n",
    "    \n",
    "    # Process each block separately\n",
    "    for _, block in unique_blocks.iterrows():\n",
    "        # Get all rows for this professor and course\n",
    "        mask = (\n",
    "            (df['Asig. Par.'] == block['Asig. Par.']) & \n",
    "            (df['Actividad'] == block['Actividad']) & \n",
    "            (df['RUT'] == block['RUT'])\n",
    "        )\n",
    "        block_data = df[mask].copy()\n",
    "        \n",
    "        # Drop duplicates based on Dia and Detalle Hora within this block\n",
    "        if not block_data.empty:\n",
    "            # Keep only first occurrence of each Dia + Detalle Hora combination\n",
    "            block_data = block_data.drop_duplicates(\n",
    "                subset=['Dia', 'Detalle Hora'],\n",
    "                keep='first'\n",
    "            )\n",
    "            \n",
    "            # Append the processed block to the result\n",
    "            processed_df = pd.concat([processed_df, block_data], ignore_index=True)\n",
    "    \n",
    "    # Add any remaining rows that weren't part of any professor block\n",
    "    mask_no_prof = df['RUT'].isna()\n",
    "    remaining_rows = df[mask_no_prof]\n",
    "    processed_df = pd.concat([processed_df, remaining_rows], ignore_index=True)\n",
    "    \n",
    "    # Sort to maintain original order as much as possible\n",
    "    if 'index' in processed_df.columns:\n",
    "        processed_df = processed_df.sort_values('index').drop('index', axis=1)\n",
    "    \n",
    "    # Write the modified DataFrame to a new CSV file\n",
    "    processed_df.to_csv('outputN6_Guia_FIA.csv', sep=';', index=False, encoding='utf-8')\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "# Execute the function\n",
    "if __name__ == \"__main__\":\n",
    "    result_df = process_schedule()\n",
    "    print(\"CSV file has been processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been processed successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def merge_duplicate_rows():\n",
    "    # Read the output from the previous step\n",
    "    df = pd.read_csv('outputN6_Guia_FIA.csv', delimiter=';', encoding='utf-8')\n",
    "    \n",
    "    # Columns to drop\n",
    "    drop_columns = ['Dia', 'Blo', 'Detalle Hora']\n",
    "    df = df.drop(columns=drop_columns, errors='ignore')\n",
    "    \n",
    "    # Group by all the specified columns, this will only merge rows that are\n",
    "    # identical in all these columns\n",
    "    check_columns = [\n",
    "        'Asig. Par.', 'Asig. Nombre', 'Vacantes', 'Inscritos', \n",
    "        'Carreras', 'Actividad', 'RUT', 'Profesor_Nombre'\n",
    "    ]\n",
    "    \n",
    "    # Group and sum hours\n",
    "    result_df = df.groupby(\n",
    "        by=check_columns,\n",
    "        dropna=False\n",
    "    ).agg({\n",
    "        'Sala': 'first',\n",
    "        'Capacidad': 'first',\n",
    "        'Cantidad Horas': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Write the modified DataFrame to a new CSV file\n",
    "    result_df.to_csv('outputN7_Guia_FIA.csv', sep=';', index=False, encoding='utf-8')\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Execute the function\n",
    "if __name__ == \"__main__\":\n",
    "    result_df = merge_duplicate_rows()\n",
    "    print(\"CSV file has been processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chek Asigments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique courses: 295\n",
      "Unique courses: 295\n",
      "Unique Asignaturas: 175\n",
      "Unique Asignaturas: 175\n"
     ]
    }
   ],
   "source": [
    "#Cuantas \"Asig. Par.\" Unicas hay en outputN7_Guia_FIA.csv\n",
    "df = pd.read_csv('outputN7_Guia_FIA.csv', delimiter=';', encoding='utf-8')\n",
    "unique_courses = df['Asig. Par.'].nunique()\n",
    "print(f\"Unique courses: {unique_courses}\")\n",
    "\n",
    "#Cuantas \"Asig. Par.\" Unicas hay en outputN6_Guia_FIA.csv\n",
    "df = pd.read_csv('outputN6_Guia_FIA.csv', delimiter=';', encoding='utf-8')\n",
    "unique_courses = df['Asig. Par.'].nunique()\n",
    "print(f\"Unique courses: {unique_courses}\")\n",
    "\n",
    "#Cuantas \"Asig. Nombre\" Unicas hay en outputN7_Guia_FIA.csv\n",
    "df = pd.read_csv('outputN7_Guia_FIA.csv', delimiter=';', encoding='utf-8')\n",
    "unique_courses = df['Asig. Nombre'].nunique()\n",
    "print(f\"Unique Asignaturas: {unique_courses}\")\n",
    "\n",
    "#Cuantas \"Asig. Nombre\" Unicas hay en outputN6_Guia_FIA.csv\n",
    "df = pd.read_csv('outputN6_Guia_FIA.csv', delimiter=';', encoding='utf-8')\n",
    "unique_courses = df['Asig. Nombre'].nunique()\n",
    "print(f\"Unique Asignaturas: {unique_courses}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed successfully!\n"
     ]
    }
   ],
   "source": [
    "#TODO: Revisar el campus\n",
    "def extract_level_from_code(code):\n",
    "    \"\"\"Extract the level number from the course code.\"\"\"\n",
    "    if pd.isna(code) or code == '':\n",
    "        return 1\n",
    "        \n",
    "    # Use regex to find the first number after letters\n",
    "    match = re.search(r'[A-Z]+(\\d)', code)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return 1  # Default value if no number is found\n",
    "\n",
    "def convert_csv_to_json(input_csv='outputN7_Guia_FIA.csv', output_json=\"../agent_input/inputOfProfesores.json\"):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_csv, delimiter=';', encoding='utf-8')\n",
    "    \n",
    "    # Sort the DataFrame by RUT and Carreras\n",
    "    df = df.sort_values(['RUT', 'Carreras'])\n",
    "    \n",
    "    # Initialize the result list and turno counter\n",
    "    profesores = []\n",
    "    turno_counter = 1\n",
    "    current_rut = None\n",
    "    \n",
    "    # Group by RUT to process each professor's courses\n",
    "    for rut, group in df.groupby('RUT'):\n",
    "        # Skip empty RUTs\n",
    "        if pd.isna(rut) or rut == '':\n",
    "            continue\n",
    "            \n",
    "        # Get the first occurrence of professor data\n",
    "        first_row = group.iloc[0]\n",
    "        \n",
    "        # Create profesor object\n",
    "        profesor = {\n",
    "            \"RUT\": rut,\n",
    "            \"Nombre\": first_row['Profesor_Nombre'],\n",
    "            \"Turno\": turno_counter,\n",
    "            \"Asignaturas\": []\n",
    "        }\n",
    "        \n",
    "        # Process each course for the professor\n",
    "        for _, row in group.iterrows():\n",
    "            asignatura = {\n",
    "                \"CodigoAsignatura\": row['Asig. Par.'],\n",
    "                \"Nombre\": row['Asig. Nombre'],\n",
    "                \"Nivel\": extract_level_from_code(row['Asig. Par.']),  # Extract level from code\n",
    "                \"Paralelo\": \"A\",  # Static value\n",
    "                \"Horas\": int(row['Cantidad Horas']),\n",
    "                \"Vacantes\": int(row['Inscritos']) if pd.notna(row['Inscritos']) else 0,\n",
    "                \"Campus\": \"Playa Brava\"  # Static value\n",
    "            }\n",
    "            profesor[\"Asignaturas\"].append(asignatura)\n",
    "        \n",
    "        profesores.append(profesor)\n",
    "        turno_counter += 1\n",
    "    \n",
    "    # Write to JSON file\n",
    "    with open(output_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump(profesores, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return profesores\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        result = convert_csv_to_json()\n",
    "        print(\"Conversion completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion completed successfully!\n"
     ]
    }
   ],
   "source": [
    "#TODO: Revisar el campus\n",
    "def convert_classrooms_to_json(input_csv='outputN7_Guia_FIA.csv', output_json=\"../agent_input/inputOfSala.json\"):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_csv, delimiter=';', encoding='utf-8')\n",
    "    \n",
    "    # Get unique classrooms and their capacities\n",
    "    # Filter out empty values and 'VIRTU'\n",
    "    classrooms_df = df[['Sala', 'Capacidad']].copy()\n",
    "    classrooms_df = classrooms_df[\n",
    "        classrooms_df['Sala'].notna() & \n",
    "        (classrooms_df['Sala'] != '') & \n",
    "        (classrooms_df['Sala'] != 'VIRTU')\n",
    "    ].drop_duplicates()\n",
    "    \n",
    "    # Sort by classroom code\n",
    "    classrooms_df = classrooms_df.sort_values('Sala')\n",
    "    \n",
    "    # Create the list of classrooms with sequential turno\n",
    "    classrooms = []\n",
    "    turno_counter = 1\n",
    "    \n",
    "    for _, row in classrooms_df.iterrows():\n",
    "        classroom = {\n",
    "            \"Turno\": turno_counter,\n",
    "            \"Codigo\": row['Sala'],\n",
    "            \"Capacidad\": int(row['Capacidad']) if pd.notna(row['Capacidad']) else 0,\n",
    "            \"Campus\": \"Playa Brava\"\n",
    "        }\n",
    "        classrooms.append(classroom)\n",
    "        turno_counter += 1\n",
    "    \n",
    "    # Write to JSON file\n",
    "    with open(output_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump(classrooms, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return classrooms\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        result = convert_classrooms_to_json()\n",
    "        print(\"Conversion completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cantidad total de asignaturas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de asignaturas diferentes: 294\n"
     ]
    }
   ],
   "source": [
    "def count_unique_subjects(filename):\n",
    "    # Read JSON file\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Create a set to store unique subject codes\n",
    "    unique_subjects = set()\n",
    "    \n",
    "    # Iterate through each professor and their subjects\n",
    "    for profesor in data:\n",
    "        for asignatura in profesor['Asignaturas']:\n",
    "            unique_subjects.add(asignatura['CodigoAsignatura'])\n",
    "    \n",
    "    return len(unique_subjects)\n",
    "\n",
    "# Use the function\n",
    "result = count_unique_subjects('profesores_input.json')\n",
    "print(f\"Número de asignaturas diferentes: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
